{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_task_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH6CAFlub-IT"
      },
      "source": [
        "## Library Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viAvvRJ6yNjW"
      },
      "source": [
        "transformers version > 3.0.0 is needed to load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lasyPueYCUt2",
        "outputId": "25ae3190-c844-41bc-a333-6bc13e0c1da6"
      },
      "source": [
        "!pip install transformers==3.0.0\r\n",
        "!pip install nlp==0.2.0\r\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: nlp==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (3.0.12)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp==0.2.0) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp==0.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp==0.2.0) (2020.12.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIQu8TJYCcYw"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import transformers\r\n",
        "import nlp\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.INFO)\r\n",
        "from datasets import load_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMIak4JYcCyJ"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haqMLO3NC2bg",
        "outputId": "2cf4cd67-7b8a-481d-839d-e373b0236e30"
      },
      "source": [
        "data = load_dataset(r'/content/comsen_loader.py')\r\n",
        "dataset_dict = {\"commonsense_qa\": data}  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default\n",
            "WARNING:datasets.builder:Reusing dataset commonsense_qa (/root/.cache/huggingface/datasets/commonsense_qa/default/1.0.0/a80a7f8888dec47d184d6dde7536608bbc756b74132340f2eb5c7bd9dde215c8)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDe6qfEBdmVZ",
        "outputId": "d398e0a6-f5ff-4649-d00a-fa7480bcd269"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['answerKey', 'statement', 'choices'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['answerKey', 'statement', 'choices'],\n",
              "        num_rows: 997\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['answerKey', 'statement', 'choices'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZwDF5hScFeS"
      },
      "source": [
        "## Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP4YfAgaCcWL",
        "outputId": "b2282857-23af-433e-e276-fe319aa717e4"
      },
      "source": [
        "# loading the saved model \r\n",
        "model = transformers.AutoModelForMultipleChoice.from_pretrained(\r\n",
        "                pretrained_model_name_or_path = \"/content/drive/MyDrive/model\", \r\n",
        "                config=transformers.AutoConfig.from_pretrained('roberta-base'),\r\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file /content/drive/MyDrive/model/pytorch_model.bin\n",
            "WARNING:transformers.modeling_utils:Some weights of the model checkpoint at /content/drive/MyDrive/model were not used when initializing RobertaForMultipleChoice: ['taskmodels_dict.commonsense_qa.roberta.embeddings.word_embeddings.weight', 'taskmodels_dict.commonsense_qa.roberta.embeddings.position_embeddings.weight', 'taskmodels_dict.commonsense_qa.roberta.embeddings.token_type_embeddings.weight', 'taskmodels_dict.commonsense_qa.roberta.embeddings.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.embeddings.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.0.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.1.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.2.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.3.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.4.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.5.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.6.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.7.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.8.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.9.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.10.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.query.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.query.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.key.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.key.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.value.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.self.value.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.intermediate.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.intermediate.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.output.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.output.dense.bias', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.output.LayerNorm.weight', 'taskmodels_dict.commonsense_qa.roberta.encoder.layer.11.output.LayerNorm.bias', 'taskmodels_dict.commonsense_qa.roberta.pooler.dense.weight', 'taskmodels_dict.commonsense_qa.roberta.pooler.dense.bias', 'taskmodels_dict.commonsense_qa.classifier.weight', 'taskmodels_dict.commonsense_qa.classifier.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.embeddings.LayerNorm.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.6.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.intermediate.dense.weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.6.output.dense.weight', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.7.intermediate.dense.bias', 'encoder.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.8.attention.self.key.bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.8.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.8.output.dense.bias', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.9.attention.output.dense.weight', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.self.query.weight', 'encoder.encoder.layer.10.attention.self.query.bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.10.output.dense.bias', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.11.attention.self.query.bias', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.11.attention.self.key.bias', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'encoder.encoder.layer.11.output.dense.weight', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_utils:Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at /content/drive/MyDrive/model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPlZIA8RcIjx"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVGF62aBcMh6"
      },
      "source": [
        "### example from the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "91kiQqV5CcSy",
        "outputId": "0250a9e4-05e6-4657-f6e6-f7c16db10fe4"
      },
      "source": [
        "from transformers import RobertaTokenizer\r\n",
        "model.eval()\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\r\n",
        "list=[]\r\n",
        "for i in range(len(data['test'])):\r\n",
        "  prompt_test = data['test'][i]['statement']\r\n",
        "  choice0_test = data['test'][i]['choices']['text'][0]\r\n",
        "  choice1_test = data['test'][i]['choices']['text'][1]\r\n",
        "  choice2_test = data['test'][i]['choices']['text'][2]\r\n",
        "\r\n",
        "  labels = torch.tensor(0).unsqueeze(0) \r\n",
        "\r\n",
        "  encoding = tokenizer([prompt_test, prompt_test, prompt_test], \r\n",
        "                     [choice0_test, choice1_test, choice2_test],\r\n",
        "                     return_tensors='pt', padding=True)\r\n",
        "  outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels,)  # batch size is 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-3fa86e15ff96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjwsFR1WF6rI",
        "outputId": "01f5245b-087a-4f3e-c343-ef7c2be4c6ff"
      },
      "source": [
        "encoding['input_ids']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,   894,   342,    10, 13280,    88,     5,  5730,     2,     2,\n",
              "           642, 15409,    32,  2333,  6907,   150, 31592,    32,  2333,  4334,\n",
              "             2],\n",
              "        [    0,   894,   342,    10, 13280,    88,     5,  5730,     2,     2,\n",
              "           102, 13280,    16,   203,  2671,    87,    10,  5730,     2,     1,\n",
              "             1],\n",
              "        [    0,   894,   342,    10, 13280,    88,     5,  5730,     2,     2,\n",
              "           642, 15409,  1395,   304,    10,  5730,     2,     1,     1,     1,\n",
              "             1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxetkbMw9SBh",
        "outputId": "ba0f1283-1ec5-4bfd-94d9-149375342b3b"
      },
      "source": [
        "data['test'][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answerKey': 'B',\n",
              " 'choices': {'label': ['A', 'B', 'C'],\n",
              "  'text': ['pigs are usually pink while pans are usually silver',\n",
              "   'a pig is much bigger than a pan',\n",
              "   'pigs cannot use a pan']},\n",
              " 'statement': 'He put a pig into the pan'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo-95eX4gU8P",
        "outputId": "07a76c05-4bd1-4d8f-a0f7-e651eb0e3b24"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.1069, grad_fn=<NllLossBackward>),\n",
              " tensor([[0.2693, 0.2609, 0.2526]], grad_fn=<ViewBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwym3Uumeot7",
        "outputId": "86c2dd11-d398-4e96-887f-2abfbbce2c2f"
      },
      "source": [
        "reasons= [choice0_test,choice1_test,choice2_test]\r\n",
        "a = [outputs[1][0][0],outputs[1][0][1],outputs[1][0][2]]\r\n",
        "m = max(a)\r\n",
        "id = [i for i, j in enumerate(a) if j == m]\r\n",
        "choice_index = id[0]\r\n",
        "print('statement : ',prompt_test)\r\n",
        "print('the right reason :',reasons[choice_index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "statement :  The chef put extra lemons on the pizza.\n",
            "the right reason : Many types of lemons are to sour to eat.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ioRri34Gy01f",
        "outputId": "43463585-c3f9-431f-e955-7ad1cf1c8c0c"
      },
      "source": [
        "reasons[choice_index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It is eathen with a napkin.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkMgtjObcUlP"
      },
      "source": [
        "### New Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBmOLFMWcaNi"
      },
      "source": [
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\r\n",
        "choice0 = \"It is eaten with a fork and a knife.\"\r\n",
        "choice1 = \"It is eaten while held in the hand.\"\r\n",
        "choice2 = \"It is eathen with a napkin.\"\r\n",
        "\r\n",
        "labels = torch.tensor(0).unsqueeze(0) \r\n",
        "\r\n",
        "encoding = tokenizer([prompt, prompt, prompt], \r\n",
        "                     [choice0, choice1, choice2],\r\n",
        "                     return_tensors='pt', padding=True)\r\n",
        "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0rsNlUrifjC",
        "outputId": "b689ca03-6c1a-45ad-aed6-a9cbca7b8fd2"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.0991, grad_fn=<NllLossBackward>),\n",
              " tensor([[0.6448, 0.6393, 0.6519]], grad_fn=<ViewBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpcgjVwMDkte",
        "outputId": "0ecce0db-de1f-4117-de3a-135dd9e46cbc"
      },
      "source": [
        "reasons= [choice0,choice1,choice2]\r\n",
        "a = [outputs[1][0][0],outputs[1][0][1],outputs[1][0][2]]\r\n",
        "m = max(a)\r\n",
        "id = [i for i, j in enumerate(a) if j == m]\r\n",
        "choice_index = id[0]\r\n",
        "print('statement : ',prompt)\r\n",
        "print('the right reason :',reasons[choice_index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "statement :  In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\n",
            "the right reason : It is eathen with a napkin.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phNIGsckypDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}